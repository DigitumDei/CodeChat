# define the models to be used for various tests

# openai
@gptprovider="openai"
@gpt-4o="gpt-4o"
@gpt-o4-mini="o4-mini"

# anthropic
@claudeprovider="anthropic"
@claude-37="claude-3-7-sonnet-latest"
@claude-35-haiku="claude-3-5-haiku-latest"

# google
@geminiprovider="google"
@gemini-25="gemini-2.5-pro-preview-03-25"
@gemini-flash-25="gemini-2.5-flash-preview-04-17"

POST  http://localhost:16005/query
Content-Type: application/json

{
    "provider":{{claudeprovider}}, 
    "model":{{claude-35-haiku}}, 
    "history":[{
        "role":"user",
        "content":"Hello, my name is Dion, I'm trying to test you memory."
    },
    {   
        "role": "assistant",
        "content": "Hello, my name is Code Chat, how may I help?"
    }], 
    "message":"What is my name and what am I doing? What are you tasked with?"
}

### stream
# Currently streaming doesn't display well, use curl to test
# curl -X POST   -H "Content-Type: application/json"   -H "Accept: text/event-stream"   -d '{
#     "provider":"openai",
#     "model":"gpt-4o",
#     "history":[],
#     "message":"Tell me a two paragraph story."
#   }'   -N   http://localhost:16005/query?stream=true

POST  http://localhost:16005/query?stream=true
Content-Type: application/json
Accept: text/event-stream

{
    "provider":{{claudeprovider}}, 
    "model":{{claude-37}}, 
    "history":[], 
    "message":"In lmm_router.py, who calls def route(self, req: QueryRequest) and where does that get routed to?",
    "files":["/workspace/daemon/codechat/llm_router.py"]
}


### Bad JSON â†’ 422 with VALIDATION_ERR
POST http://localhost:16005/query
Content-Type: application/json

{"foo": "bar"}

### Health endpoint still works
GET http://localhost:16005/health